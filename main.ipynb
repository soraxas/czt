{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb8129e",
   "metadata": {},
   "source": [
    "# Credit card fraud detection with Federated XGBoost\n",
    "\n",
    "This notebook shows how to convert an existing tabular credit dataset, enrich and pre-process the data using a single site (like a centralized dataset), and then convert this centralized process into federated ETL steps easily. Then, construct a federated XGBoost; the only thing the user needs to define is the XGBoost data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23442c",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation \n",
    "First, we prepare the data by adding random transactional information to the base creditcard dataset following the below script:\n",
    "\n",
    "* [prepare data](./notebooks/1.1.prepare_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef04d3",
   "metadata": {},
   "source": [
    "## Step 2: Feature Analysis\n",
    "\n",
    "For this stage, we would like to analyze the data, understand the features, and derive (and encode) secondary features that can be more useful for building the model.\n",
    "\n",
    "Towards this goal, there are two options:\n",
    "1. **Feature Enrichment**: This process involves adding new features based on the existing data. For example, we can calculate the average transaction amount for each currency and add this as a new feature. \n",
    "2. **Feature Encoding**: This process involves encoding the current features and transforming them to embedding space via machine learning models. This model can be either pre-trained, or trained with the candidate dataset.\n",
    "\n",
    "Considering the fact that the only two numerical features in the dataset are \"Amount\" and \"Time\", we will perform feature enrichment first. Optionally, we can also perform feature encoding. In this example, we use a graph neural network (GNN); we will train the GNN model in a federated, unsupervised fashion and then use the model to encode the features for all sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22363b",
   "metadata": {},
   "source": [
    "### Step 2.1: Rule-based Feature Enrichment\n",
    "\n",
    "#### Single-site Enrichment and Additional Processing\n",
    "The detailed feature enrichment step is illustrated using one site as example: \n",
    "\n",
    "* [feature_enrichments with-one-site](./notebooks/2.1.1.feature_enrichment.ipynb)\n",
    "\n",
    "Similarly, we examine the additional pre-processing step using one site: \n",
    "\n",
    "* [pre-processing with one-site](./notebooks/2.1.2.pre_process.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e98f8d",
   "metadata": {},
   "source": [
    "#### Federated Job to Perform on All Sites\n",
    "In order to run feature enrichment and processing job on each site similar to above steps, we wrote federated ETL job scripts for client-side based on single-site implementations.\n",
    "\n",
    "* [enrichment script](./src/enrich.py)\n",
    "* [pre-processing script](./src/pre_process.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140947e",
   "metadata": {},
   "source": [
    "### (Optional) Step 2.2: GNN-based Feature Encoding\n",
    "Based on raw features, or combining the derived features from **Step 2.1**, we can use machine learning models to encode the features. \n",
    "In this example, we use federated GNN to learn and generate the feature embeddings.\n",
    "\n",
    "First, we construct a graph based on the transaction data. Each node represents a transaction, and the edges represent the relationships between transactions. We then use the GNN to learn the embeddings of the nodes, which represent the transaction features.\n",
    "\n",
    "#### Single-site operation example: graph construction\n",
    "The detailed graph construction step is illustrated using one site as example:\n",
    "\n",
    "* [graph_construction with one-site](./notebooks/graph_construct.ipynb)\n",
    "\n",
    "The detailed GNN training and encoding step is illustrated using one site as example:\n",
    "\n",
    "* [gnn_training_encoding with one-site](./notebooks/gnn_train_encode.ipynb)\n",
    "\n",
    "#### Federated Job to Perform on All Sites\n",
    "In order to run feature graph construction job on each site similar to the enrichment and processing steps, we wrote federated ETL job scripts for client-side based on single-site implementations.\n",
    "\n",
    "* [graph_construction script](./src/graph_construct.py)\n",
    "* [gnn_train_encode script](./src/gnn_train_encode.py)\n",
    "\n",
    "\n",
    "The resulting GNN encodings will be merged with the normalized data for enhancing the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1c223",
   "metadata": {},
   "source": [
    "## Step 3: Federated XGBoost \n",
    "\n",
    "Now that we have the data ready, either enriched and normalized features, or GNN feature embeddings, we can fit them with XGBoost. NVIDIA FLARE has already written XGBoost Controller and Executor for the job. All we need to provide is the data loader to fit into the XGBoost.\n",
    "\n",
    "Notice we assign defined a [```CreditCardDataLoader```](./src/xgb_data_loader.py), this a XGBLoader we defined to load the credit card dataset. \n",
    "\n",
    "```py\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost.core import DataSplitMode\n",
    "\n",
    "from src.app_opt.xgboost.data_loader import XGBDataLoader\n",
    "\n",
    "\n",
    "class CreditCardDataLoader(XGBDataLoader):\n",
    "    def __init__(self, root_dir: str, file_postfix: str):\n",
    "        self.dataset_names = [\"train\", \"test\"]\n",
    "        self.base_file_names = {}\n",
    "        self.root_dir = root_dir\n",
    "        self.file_postfix = file_postfix\n",
    "        for name in self.dataset_names:\n",
    "            self.base_file_names[name] = name + file_postfix\n",
    "        self.numerical_columns = [\n",
    "            \"Timestamp\",\n",
    "            \"Amount\",\n",
    "            \"trans_volume\",\n",
    "            \"total_amount\",\n",
    "            \"average_amount\",\n",
    "            \"hist_trans_volume\",\n",
    "            \"hist_total_amount\",\n",
    "            \"hist_average_amount\",\n",
    "            \"x2_y1\",\n",
    "            \"x3_y2\",\n",
    "        ]\n",
    "\n",
    "    def load_data(self, client_id: str, split_mode: int) -> Tuple[xgb.DMatrix, xgb.DMatrix]:\n",
    "        data = {}\n",
    "        for ds_name in self.dataset_names:\n",
    "            print(\"\\nloading for site = \", client_id, f\"{ds_name} dataset \\n\")\n",
    "            file_name = os.path.join(self.root_dir, client_id, self.base_file_names[ds_name])\n",
    "            df = pd.read_csv(file_name)\n",
    "            data_num = len(data)\n",
    "\n",
    "            # split to feature and label\n",
    "            y = df[\"Class\"]\n",
    "            x = df[self.numerical_columns]\n",
    "            data[ds_name] = (x, y, data_num)\n",
    "\n",
    "\n",
    "        # training\n",
    "        x_train, y_train, total_train_data_num = data[\"train\"]\n",
    "        data_split_mode = DataSplitMode(split_mode)\n",
    "        dmat_train = xgb.DMatrix(x_train, label=y_train, data_split_mode=data_split_mode)\n",
    "\n",
    "        # validation\n",
    "        x_valid, y_valid, total_valid_data_num = data[\"test\"]\n",
    "        dmat_valid = xgb.DMatrix(x_valid, label=y_valid, data_split_mode=data_split_mode)\n",
    "\n",
    "        return dmat_train, dmat_valid\n",
    "```\n",
    "\n",
    "We are now ready to run all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c8e91",
   "metadata": {},
   "source": [
    "## Run All the Jobs End-to-end\n",
    "Here we are going to run each job in sequence. For real-world use case,\n",
    "\n",
    "* prepare data is not needed, as you already have the data\n",
    "* feature enrichment / encoding scripts need to be defined based on your own technique\n",
    "* for XGBoost Job, you will need to write your own data loader \n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samayashar/fraud-detection-transactions-dataset\")\n",
    "input_csv = f\"{path}/synthetic_fraud_dataset.csv\"\n",
    "\n",
    "\n",
    "# only generate config file, or also run the simulated job (on the same machine)\n",
    "config_only = False\n",
    "# the workdir is used to store the job config and the simulated job results for each node\n",
    "work_dir = \"/tmp/czt/jobs/workdir\"\n",
    "# the processed dataset folder is used to store the processed data, preparing for each node, and also output the results\n",
    "output_folder = \"/tmp/czt/dataset\"\n",
    "\n",
    "!mkdir -p {output_folder}\n",
    "!mkdir -p {output_folder}\n",
    "\n",
    "import sys\n",
    "PY = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! {PY} ./utils/prepare_data.py -i {input_csv} -o {output_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e102986",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names = [\n",
    "    \"HCBHSGSG_Bank_9\",\n",
    "    \"XITXUS33_Bank_10\",\n",
    "    \"YSYCESMM_Bank_7\",\n",
    "    \"YXRXGB22_Bank_3\",\n",
    "    \"ZNZZAU3M_Bank_8\",\n",
    "]\n",
    "\n",
    "!echo {' '.join(site_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35887ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare import FedJob\n",
    "from nvflare.app_common.workflows.etl_controller import ETLController\n",
    "from nvflare.job_config.script_runner import ScriptRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36557a57",
   "metadata": {},
   "source": [
    "### Enrich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4bd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job = FedJob(name=\"enrich_job\")\n",
    "\n",
    "enrich_ctrl = ETLController(task_name=\"enrich\")\n",
    "job.to(enrich_ctrl, \"server\", id=\"enrich\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = ScriptRunner(\n",
    "        # for this, we output the enriched data to the same folder\n",
    "        script=\"src/enrich.py\", script_args=f\"-i {output_folder} -o {output_folder}\"\n",
    "    )\n",
    "    job.to(executor, site_name, tasks=[\"enrich\"])\n",
    "\n",
    "if work_dir:\n",
    "    print(f\"{work_dir=}\")\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7ac67",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32ff6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job = FedJob(name=\"pre_processing_job\")\n",
    "\n",
    "pre_process_ctrl = ETLController(task_name=\"pre_process\")\n",
    "job.to(pre_process_ctrl, \"server\", id=\"pre_process\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = ScriptRunner(script=\"src/pre_process.py\", script_args=f\"-i {output_folder} -o {output_folder}\")\n",
    "    job.to(executor, site_name, tasks=[\"pre_process\"])\n",
    "\n",
    "if work_dir:\n",
    "    print(f\"{work_dir=}\")\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70dacc",
   "metadata": {},
   "source": [
    "### Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33910d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job = FedJob(name=\"graph_construct_job\")\n",
    "\n",
    "graph_construct_ctrl = ETLController(task_name=\"graph_construct\")\n",
    "job.to(graph_construct_ctrl, \"server\", id=\"graph_construct\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = ScriptRunner(script=\"src/graph_construct.py\", script_args=f\"-i {output_folder} -o {output_folder}\")\n",
    "    job.to(executor, site_name, tasks=[\"graph_construct\"])\n",
    "\n",
    "if work_dir:\n",
    "    print(f\"{work_dir=}\")\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1d89f",
   "metadata": {},
   "source": [
    "### GNN Training and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca593eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphSAGE\n",
    "\n",
    "from nvflare import FedJob\n",
    "from nvflare.app_common.workflows.fedavg import FedAvg\n",
    "from nvflare.app_opt.pt.job_config.model import PTModel\n",
    "from nvflare.job_config.script_runner import ScriptRunner\n",
    "\n",
    "job = FedJob(name=\"gnn_train_encode_job\")\n",
    "\n",
    "# Define the controller workflow and send to server\n",
    "controller = FedAvg(\n",
    "    num_clients=len(site_names),\n",
    "    num_rounds=100,\n",
    ")\n",
    "job.to(controller, \"server\")\n",
    "\n",
    "# Define the model\n",
    "model = GraphSAGE(\n",
    "    in_channels=10,\n",
    "    hidden_channels=64,\n",
    "    num_layers=2,\n",
    "    out_channels=64,\n",
    ")\n",
    "job.to(PTModel(model), \"server\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = ScriptRunner(script=\"src/gnn_train_encode.py\", script_args=f\"-i {output_folder} -o {output_folder}\")\n",
    "    job.to(executor, site_name)\n",
    "\n",
    "if work_dir:\n",
    "    print(f\"{work_dir=}\")\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f0b44",
   "metadata": {},
   "source": [
    "### GNN Encoding Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "! {PY} ./utils/merge_feat.py -i {output_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4f105",
   "metadata": {},
   "source": [
    "### Run XGBoost Job\n",
    "#### Without GNN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95253c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare.app_opt.xgboost.histogram_based_v2.fed_controller import XGBFedController\n",
    "from nvflare.app_opt.xgboost.histogram_based_v2.fed_executor import (\n",
    "    FedXGBHistogramExecutor,\n",
    ")\n",
    "\n",
    "from xgb_data_loader import CreditCardDataLoader\n",
    "\n",
    "\n",
    "num_rounds = 10\n",
    "early_stopping_rounds = 10\n",
    "xgb_params = {\n",
    "    \"max_depth\": 8,\n",
    "    \"eta\": 0.1,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"nthread\": 16,\n",
    "}\n",
    "\n",
    "job = FedJob(name=\"xgb_job\")\n",
    "\n",
    "# Define the controller workflow and send to server\n",
    "controller = XGBFedController(\n",
    "    num_rounds=num_rounds,\n",
    "    data_split_mode=0,\n",
    "    secure_training=False,\n",
    "    xgb_params=xgb_params,\n",
    "    xgb_options={\"early_stopping_rounds\": early_stopping_rounds},\n",
    ")\n",
    "job.to(controller, \"server\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = FedXGBHistogramExecutor(data_loader_id=\"data_loader\")\n",
    "    job.to(executor, site_name)\n",
    "    data_loader = CreditCardDataLoader(root_dir=output_folder, file_postfix=\"_normalized.csv\")\n",
    "    job.to(data_loader, site_name, id=\"data_loader\")\n",
    "\n",
    "if work_dir:\n",
    "    print(\"work_dir=\", work_dir)\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa38c17",
   "metadata": {},
   "source": [
    "#### With GNN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgb_embed_data_loader import CreditCardEmbedDataLoader\n",
    "\n",
    "from nvflare import FedJob\n",
    "from nvflare.app_opt.xgboost.histogram_based_v2.fed_controller import XGBFedController\n",
    "from nvflare.app_opt.xgboost.histogram_based_v2.fed_executor import (\n",
    "    FedXGBHistogramExecutor,\n",
    ")\n",
    "\n",
    "num_rounds = 10\n",
    "early_stopping_rounds = 10\n",
    "xgb_params = {\n",
    "    \"max_depth\": 8,\n",
    "    \"eta\": 0.1,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"nthread\": 16,\n",
    "}\n",
    "\n",
    "job = FedJob(name=\"xgb_job_embed\")\n",
    "\n",
    "# Define the controller workflow and send to server\n",
    "controller = XGBFedController(\n",
    "    num_rounds=num_rounds,\n",
    "    data_split_mode=0,\n",
    "    secure_training=False,\n",
    "    xgb_params=xgb_params,\n",
    "    xgb_options={\"early_stopping_rounds\": early_stopping_rounds},\n",
    ")\n",
    "job.to(controller, \"server\")\n",
    "\n",
    "# Add clients\n",
    "for site_name in site_names:\n",
    "    executor = FedXGBHistogramExecutor(data_loader_id=\"data_loader\")\n",
    "    job.to(executor, site_name)\n",
    "    data_loader = CreditCardEmbedDataLoader(\n",
    "        root_dir=output_folder, file_postfix=\"_combined.csv\"\n",
    "    )\n",
    "    job.to(data_loader, site_name, id=\"data_loader\")\n",
    "\n",
    "if work_dir:\n",
    "    print(\"work_dir=\", work_dir)\n",
    "    job.export_job(work_dir)\n",
    "\n",
    "if not config_only:\n",
    "    job.simulator_run(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e38ec",
   "metadata": {},
   "source": [
    "## Prepare Job for POC and Production\n",
    "\n",
    "With job running well in simulator, we are ready to run in a POC mode, so we can simulate the deployment in localhost or simply deploy to production. \n",
    "\n",
    "All we need is the job definition; we can use the job.export_job() method to generate the job configuration and export it to a given directory. For example, in xgb_job.py, we have the following\n",
    "\n",
    "```\n",
    "    if work_dir:\n",
    "        print(\"work_dir=\", work_dir)\n",
    "        job.export_job(work_dir)\n",
    "\n",
    "    if not args.config_only:\n",
    "        job.simulator_run(work_dir)\n",
    "```\n",
    "\n",
    "let's try this out and then look at the directory. We use ```tree``` command if you have it. othewise, simply use ```ls -al ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {work_dir} -type f -path \"*/simulate_job/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ac88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /tmp/czt/jobs/workdir/server/simulate_job/meta.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57961cb",
   "metadata": {},
   "source": [
    "Now we have the job definition, you can either run it in POC mode or production setup. \n",
    "\n",
    "* setup POC\n",
    "``` \n",
    "    nvfalre poc prepare -c <list of clients>\n",
    "    nvflare poc start -ex admin@nvidia.com  \n",
    "```\n",
    "  \n",
    "* submit job using NVFLARE console \n",
    "        \n",
    "    from different terminal \n",
    "   \n",
    "   ```\n",
    "   nvflare poc start -p admin@nvidia.com\n",
    "   ```\n",
    "   using submit job command\n",
    "    \n",
    "* use nvflare job submit command  to submit job\n",
    "\n",
    "* use NVFLARE API to submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2700e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
